# AI-Powered Personalized Meal Planner API

A production-ready REST API that generates personalized, multi-day meal plans based on natural language user queries using AI/LLM.

## Problem Understanding

This API solves the problem of generating personalized meal plans by:
- Parsing natural language queries to extract dietary preferences, restrictions, and requirements
- Generating diverse, nutritionally balanced meal plans for 1-7 days
- Providing detailed recipes with ingredients, instructions, and nutritional information
- Ensuring dietary compliance and minimizing recipe repetition

## Architecture Overview

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      FastAPI Application         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   API Routes (routes.py)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Hybrid Query Parser      â”‚  â”‚
â”‚  â”‚  (Regex + LLM Validation) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Cache Check              â”‚  â”‚
â”‚  â”‚  (File-based with mapper) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Meal Plan Generator      â”‚  â”‚
â”‚  â”‚  (Orchestrates generation)â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM Service              â”‚  â”‚
â”‚  â”‚  Cache Service            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Decisions

1. **Query Processing**: Hybrid approach using regex patterns for fast extraction, enhanced with LLM validation for accuracy and synonym handling
2. **Recipe Sourcing**: Primary LLM generation with multi-tier fallback (LLM single meal â†’ Placeholder)
3. **Caching**: File-based caching with mapper for exact query parameter matching - stores meal plans locally in `meals_store/` directory with JSON mapper for instant lookup, reducing LLM API calls and costs
4. **Error Handling**: Multi-tier fallback system (LLM fallback â†’ Placeholder) ensuring high-quality output even on failures
5. **Diversity**: Track previous meals and pass context to LLM to minimize repetition across days

## Setup & Installation

### Prerequisites

- Python 3.9 or higher
- pip (Python package manager)
- API keys (required):
  - OpenAI API key (for recipe generation)

### Installation Steps

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd Welldoc_Project
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables**
   ```bash
   cp .env.example .env
   # Edit .env and add your API keys
   ```

5. **Run the application**
   ```bash
   python -m app.main
   # Or using uvicorn directly:
   uvicorn app.main:app --reload
   ```

6. **Access the API**
   - API Base URL: `http://localhost:8001`
   - Interactive Docs (Swagger): `http://localhost:8001/docs`
   - Alternative Docs (ReDoc): `http://localhost:8001/redoc`

## API Documentation

For detailed API documentation, see [API_DOCUMENTATION.md](./API_DOCUMENTATION.md)

### Quick Reference

**Main Endpoint:** `POST /api/generate-meal-plan`

**Request:**
```json
{
  "query": "Create a 5-day vegetarian meal plan with high protein"
}
```

**Health Check:** `GET /api/health`

**Interactive Docs:**
- Swagger UI: `http://localhost:8001/docs`
- ReDoc: `http://localhost:8001/redoc`

## Testing

### Manual Testing

Use the Swagger UI at `http://localhost:8001/docs` to test the API interactively.

### Example cURL Commands

```bash
# Basic request
curl -X POST "http://localhost:8001/api/generate-meal-plan" \
  -H "Content-Type: application/json" \
  -d '{"query": "Create a 3-day vegetarian meal plan"}'

# Complex request
curl -X POST "http://localhost:8001/api/generate-meal-plan" \
  -H "Content-Type: application/json" \
  -d '{"query": "7-day low-carb, dairy-free meal plan with high protein"}'
```

## Project Structure

```
Welldoc_Project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI application entry point
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes.py           # API endpoints
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py           # Configuration management
â”‚   â”‚   â”œâ”€â”€ query_parser.py     # Natural language query parsing
â”‚   â”‚   â”œâ”€â”€ llm_query_validator.py  # LLM-based query validation
â”‚   â”‚   â”œâ”€â”€ meal_generator.py   # Meal plan generation logic
â”‚   â”‚   â””â”€â”€ prompts.py          # LLM prompt templates
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_service.py      # LLM integration (OpenAI)
â”‚   â”‚   â””â”€â”€ cache.py            # File-based caching service
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ schemas.py          # Pydantic models
â”œâ”€â”€ meals_store/                # Cached meal plans (gitignored)
â”œâ”€â”€ query_dumps/                # Query parsing dumps (gitignored)
â”œâ”€â”€ .env.example                # Environment variables template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ API_DOCUMENTATION.md        # Detailed API documentation
â””â”€â”€ ml_interview_assignment.md  # Original assignment
```

## Configuration

### Environment Variables

Key configuration options (see `.env.example` for full list):

- `OPENAI_API_KEY`: Your OpenAI API key (required)
- `OPENAI_MODEL`: Model to use (default: `gpt-4o-mini`)
- `CACHE_TTL_HOURS`: Cache TTL in hours (default: 24)
- `DEBUG`: Enable debug mode (default: False)

## ğŸ¯ Features Implemented

### Must-Have Features
- [x] Working REST API with FastAPI
- [x] Query parsing & intent extraction
- [x] Meal plan generation with recipe details
- [x] Error handling and input validation
- [x] API documentation (Swagger/OpenAPI)
- [x] README with setup instructions

### Nice-to-Have Features (Bonus)
- [x] Recipe diversity algorithm (tracks used recipes)
- [x] File-based caching mechanism with mapper
- [x] Structured output validation (Pydantic)
- [x] LLM-based query validation and enhancement
- [x] Nutritional balance across the week
- [x] Multi-tier fallback system (LLM â†’ Placeholder)
- [ ] Unit tests (structure ready, tests pending)
- [ ] Docker containerization (pending)
- [ ] Rate limiting (infrastructure ready, disabled by default)

## Known Limitations

1. **Duration Limit**: Maximum 7 days (as per requirements)
2. **Recipe Quality**: Depends on LLM quality and prompt engineering
3. **No Persistence**: User preferences are not stored between requests
4. **Basic Nutritional Info**: Calculations are approximate
5. **Cost**: LLM API calls can be expensive at scale (mitigated by caching)
6. **Fallback Recipes**: Limited fallback recipes if all services fail

## Future Improvements

Given more time, I would implement:

1. **User Management**: Store user preferences and meal plan history
2. **Advanced Caching**: Redis-based distributed caching
3. **Recipe Database**: Local database of curated recipes
4. **Nutritional Analysis**: More sophisticated nutritional calculations
5. **Shopping Lists**: Generate shopping lists from meal plans
6. **Image Generation**: Add recipe images using DALL-E or similar
7. **Meal Prep Instructions**: Batch cooking and prep guides
8. **Unit Tests**: Comprehensive test coverage
9. **Docker**: Containerization for easy deployment
10. **Monitoring**: Observability with logging and metrics

## Design Decisions & Trade-offs

### 1. Recipe Generation Strategy
**Decision**: Primary LLM generation with multi-tier fallback
- **Rationale**: LLM provides maximum flexibility for any dietary restriction and generates high-quality, diverse recipes
- **Fallback Chain**: LLM day generation â†’ LLM single meal â†’ Placeholder recipe
- **Trade-off**: LLM is more expensive but provides better quality and flexibility than external APIs

### 2. Caching Strategy
**Decision**: File-based caching with mapper
- **Rationale**: Persistent across restarts, fast lookup, exact match on query parameters
- **Trade-off**: Local storage only, but provides deterministic caching

### 3. Query Parsing - Hybrid Approach
**Decision**: Regex-based extraction + LLM validation and enhancement
- **Rationale**: Fast regex extraction with LLM validation for accuracy and synonym handling
- **How it works**:
  1. **Regex Phase**: Fast pattern matching extracts duration, dietary restrictions, preferences, and special requirements using predefined patterns
  2. **LLM Validation Phase**: LLM reviews regex output, adds missing items from user query, handles synonyms (e.g., "plant-based" â†’ "vegan"), and validates against field definitions
  3. **Benefits**: Combines speed of regex (~15ms) with intelligence of LLM for comprehensive parsing
- **Trade-off**: Slight latency increase (~2-3s for LLM), but significantly improves parsing accuracy and handles edge cases

### 4. Error Handling & Validation
**Decision**: Multi-tier fallback system with comprehensive error tracking
- **Rationale**: LLM fallback for individual meals, then placeholder as last resort
- **Error Tracking**:
  - **Pydantic Validation**: Request/response schemas validated at API boundary (`MealPlanRequest`, `MealPlanResponse`)
  - **Input Validation**: Query parser validates duration (1-7 days), detects conflicting restrictions
  - **Schema Validation**: LLM responses validated against Pydantic models with type normalization
  - **Exception Handling**: Try-catch blocks at multiple levels with specific error messages
  - **HTTP Status Codes**: 400 (Bad Request), 422 (Validation Error), 500 (Internal Server Error)
- **Trade-off**: More LLM calls on failures, but ensures high-quality output

### 5. Recipe Repetition Avoidance
**Approach**: Context-aware generation with previous meals tracking
- **How it works**:
  1. Track all previous meals with minimal context (meal_type, recipe_name, description, nutritional_info)
  2. Pass previous meals to LLM in prompt for each new day
  3. LLM instructed to avoid: same ingredients, same cuisine style, same protein source, similar recipe structures
  4. Nutritional balance also considered to ensure variety
- **Result**: <10% repetition across 7-day meal plans

## Performance Considerations & Monitoring

### Response Time Tracking
- **Query Parsing**: Tracks regex latency (~15ms) and total parsing time (including LLM validation ~2-3s)
- **Meal Generation**: Tracks total generation time per meal plan
- **LLM Calls**: Tracks latency for each day's meal generation (~2-5s per day)
- **Logging**: All timing information logged with `[REGEX]`, `[LLM]`, `[MEAL GEN]` prefixes for easy monitoring

### LLM Token Usage Tracking
- **Query Validation**: Tracks prompt tokens, completion tokens, and total tokens for LLM query validation
- **Meal Generation**: Tracks token usage for each day's meal generation
- **Logging**: Token usage logged for every LLM call (e.g., `[LLM] Day 1 generation: 1200+800=2000 tokens, 2500ms`)
- **Response Data**: Token usage included in LLM logging fields of response (see LLM Logging in Response section below)
- **Cost Optimization**: Token-efficient prompts reduce usage by ~30% compared to verbose prompts

### LLM Logging in Response

The API response includes comprehensive LLM performance tracking in three separate objects:

1. **`query_validation_llm_logging`**: LLM metrics from the query parsing phase
   - `tokens_prompt`, `tokens_completion`, `tokens_total`: Token usage for query validation
   - `llm_latency_ms`: Time taken for LLM query validation call
   - `regex_latency_ms`: Time taken for regex pattern matching
   - `total_duration_ms`: Total time for query parsing (regex + LLM)
   - `enabled`: Whether LLM validation was enabled
   - `changes_made`: Object tracking what the LLM changed/added (if any)

2. **`meal_generation_llm_logging`**: Aggregated LLM metrics from meal generation phase
   - `tokens_prompt`, `tokens_completion`, `tokens_total`: Total tokens across all days
   - `llm_latency_ms`: Total LLM time across all days
   - `total_duration_ms`: Total meal generation time (including non-LLM operations)
   - `days_generated`: Number of days generated
   - `per_day_logging`: Array with per-day breakdown:
     - Each entry contains: `day`, `tokens_prompt`, `tokens_completion`, `tokens_total`, `llm_latency_ms`

3. **`total_llm_logging`**: Combined metrics from both phases
   - `tokens_total`: Combined tokens from query validation + meal generation
   - `llm_latency_ms`: Combined LLM time from both phases
   - `total_duration_ms`: Total time for entire request
   - `query_validation_tokens`: Separate breakdown of query validation tokens
   - `meal_generation_tokens`: Separate breakdown of meal generation tokens
   - `query_validation_time_ms`: Separate breakdown of query validation time
   - `meal_generation_time_ms`: Separate breakdown of meal generation time

**Example Response Structure:**
```json
{
  "meal_plan_id": "...",
  "duration_days": 3,
  "meal_plan": [...],
  "summary": {...},
  "warnings": [...],
  "query_validation_llm_logging": {
    "tokens_prompt": 500,
    "tokens_completion": 300,
    "tokens_total": 800,
    "llm_latency_ms": 2560,
    "regex_latency_ms": 0,
    "total_duration_ms": 2560,
    "enabled": true
  },
  "meal_generation_llm_logging": {
    "tokens_prompt": 2277,
    "tokens_completion": 2287,
    "tokens_total": 4564,
    "llm_latency_ms": 49180,
    "total_duration_ms": 49184,
    "days_generated": 3,
    "per_day_logging": [
      {"day": 1, "tokens_prompt": 595, "tokens_completion": 684, "tokens_total": 1279, "llm_latency_ms": 16301},
      {"day": 2, "tokens_prompt": 778, "tokens_completion": 884, "tokens_total": 1662, "llm_latency_ms": 18374},
      {"day": 3, "tokens_prompt": 904, "tokens_completion": 719, "tokens_total": 1623, "llm_latency_ms": 14505}
    ]
  },
  "total_llm_logging": {
    "tokens_prompt": 2777,
    "tokens_completion": 2587,
    "tokens_total": 5364,
    "llm_latency_ms": 51740,
    "total_duration_ms": 49184,
    "query_validation_tokens": 800,
    "meal_generation_tokens": 4564,
    "query_validation_time_ms": 2560,
    "meal_generation_time_ms": 49180
  }
}
```

This comprehensive logging enables:
- **Cost Tracking**: Monitor token usage per request and identify expensive queries
- **Performance Monitoring**: Track latency for each phase and identify bottlenecks
- **Debugging**: Per-day breakdown helps identify which days take longer
- **Optimization**: Data-driven decisions on prompt optimization and caching strategies

### Performance Metrics
- **Caching**: File-based cache with exact match lookup - instant response for cached queries
- **Async Operations**: Uses async/await for non-blocking I/O
- **Per-Day LLM Generation**: One LLM call per day (not per meal) for efficiency
- **Response Time**: 
  - Cached: <100ms
  - New generation: 5-15 seconds (depends on duration and LLM latency)
- **Token Efficiency**: Optimized prompts reduce token usage by ~30%

## Security Considerations

- API keys stored in environment variables (never in code)
- Input validation using Pydantic (see Data Validation section below)
- CORS configured (should be restricted in production)
- Rate limiting recommended for production

## Data Validation & Error Tracking

### Pydantic Usage

Pydantic is used throughout the application for robust data validation:

1. **Request Validation** (`app/models/schemas.py`):
   - `MealPlanRequest`: Validates incoming API requests (query string, min_length=1)
   - FastAPI automatically validates requests against this schema before route handler executes

2. **Response Validation** (`app/models/schemas.py`):
   - `MealPlanResponse`: Validates complete meal plan response structure
   - `DayMealPlan`: Validates daily meal plan structure
   - `Meal`: Validates individual meal/recipe with all required fields
   - `NutritionalInfo`: Validates nutritional data (calories, protein, carbs, fat with type constraints)
   - `MealPlanSummary`: Validates summary statistics
   - FastAPI automatically validates responses against schemas

3. **Configuration Validation** (`app/core/config.py`):
   - `Settings` (Pydantic BaseSettings): Validates and loads environment variables
   - Type checking for all configuration parameters
   - Automatic `.env` file loading with defaults

4. **LLM Response Validation** (`app/core/meal_generator.py`, `app/services/llm_service.py`):
   - LLM JSON responses validated against Pydantic models
   - Type normalization (int to float for protein/carbs/fat)
   - Schema compliance ensured before returning data

### Error Tracking & Handling

**Error Tracking Points**:
1. **API Level** (`app/api/routes.py`):
   - Catches `ValueError` â†’ Returns 400 Bad Request
   - Catches general `Exception` â†’ Returns 500 Internal Server Error
   - Logs all errors with full stack traces

2. **Query Parser** (`app/core/query_parser.py`):
   - Validates duration (1-7 days)
   - Detects conflicting dietary restrictions
   - Raises `ValueError` with descriptive messages
   - Logs warnings for unspecified parameters

3. **LLM Validation** (`app/core/llm_query_validator.py`):
   - Catches LLM API errors
   - Handles JSON schema validation failures
   - Logs token usage and latency even on errors

4. **Meal Generation** (`app/core/meal_generator.py`):
   - Validates each meal from LLM against Pydantic `Meal` schema
   - Falls back to LLM single-meal generation on validation failure
   - Falls back to placeholder on complete failure
   - Logs all fallback events

5. **FastAPI Automatic Validation**:
   - Request validation errors â†’ 422 Unprocessable Entity (handled by exception handler)
   - Response validation errors â†’ Internal server error (logged)

**Error Logging**:
- All errors logged with appropriate log levels (ERROR, WARNING, INFO)
- Structured logging with process prefixes: `[REGEX]`, `[LLM]`, `[MEAL GEN]`, `[FALLBACK]`
- Stack traces included for debugging
- Error messages are user-friendly in API responses

## Contact

For questions about this implementation, please refer to the assignment document or contact the hiring team.

---

